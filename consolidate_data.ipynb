{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05673db",
   "metadata": {},
   "source": [
    "# Data Consolidation Notebook\n",
    "This notebook is structured to facilitate the data consolidation and preparation of the diary.csv for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2beb0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import requests  # to make HTTP requests\n",
    "from requests import Request, Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects  # to handle potential request errors\n",
    "import json  # to handle JSON data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c49219",
   "metadata": {},
   "source": [
    "## Initial Data Loading and Display\n",
    "The following block loads the data from a specified source and displays the first few rows. This is crucial for understanding the initial structure of the data and ensuring that it has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0195be2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rewatch</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Watched Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/3/21</td>\n",
       "      <td>Godzilla vs. Kong</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://boxd.it/1PXghH</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/3/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/19/21</td>\n",
       "      <td>The Woman in the Window</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://boxd.it/1S4UiR</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/19/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/24/21</td>\n",
       "      <td>About Time</td>\n",
       "      <td>2013</td>\n",
       "      <td>https://boxd.it/1SLTdH</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/24/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/26/21</td>\n",
       "      <td>Pretty Woman</td>\n",
       "      <td>1990</td>\n",
       "      <td>https://boxd.it/1T0ZVb</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/26/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/2/21</td>\n",
       "      <td>The Handmaiden</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://boxd.it/1TZaH5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/2/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>4/23/24</td>\n",
       "      <td>Galaxy Quest</td>\n",
       "      <td>1999</td>\n",
       "      <td>https://boxd.it/6kDYMZ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/23/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>4/25/24</td>\n",
       "      <td>Misery</td>\n",
       "      <td>1990</td>\n",
       "      <td>https://boxd.it/6lmxKR</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/25/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>4/27/24</td>\n",
       "      <td>Hot Fuzz</td>\n",
       "      <td>2007</td>\n",
       "      <td>https://boxd.it/6m6EV5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/27/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>4/27/24</td>\n",
       "      <td>Ricky Stanicky</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/6m7Xpf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/27/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>5/1/24</td>\n",
       "      <td>Jurassic World Dominion</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://boxd.it/6nPTQD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/1/24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                     Name  Year          Letterboxd URI  Rating  \\\n",
       "0     5/3/21        Godzilla vs. Kong  2021  https://boxd.it/1PXghH     3.0   \n",
       "1    5/19/21  The Woman in the Window  2021  https://boxd.it/1S4UiR     2.5   \n",
       "2    5/24/21               About Time  2013  https://boxd.it/1SLTdH     4.5   \n",
       "3    5/26/21             Pretty Woman  1990  https://boxd.it/1T0ZVb     2.5   \n",
       "4     6/2/21           The Handmaiden  2016  https://boxd.it/1TZaH5     4.0   \n",
       "..       ...                      ...   ...                     ...     ...   \n",
       "321  4/23/24             Galaxy Quest  1999  https://boxd.it/6kDYMZ     4.0   \n",
       "322  4/25/24                   Misery  1990  https://boxd.it/6lmxKR     4.5   \n",
       "323  4/27/24                 Hot Fuzz  2007  https://boxd.it/6m6EV5     4.0   \n",
       "324  4/27/24           Ricky Stanicky  2024  https://boxd.it/6m7Xpf     2.0   \n",
       "325   5/1/24  Jurassic World Dominion  2022  https://boxd.it/6nPTQD     2.0   \n",
       "\n",
       "    Rewatch Tags Watched Date  \n",
       "0       NaN  NaN       5/3/21  \n",
       "1       NaN  NaN      5/19/21  \n",
       "2       NaN  NaN      5/24/21  \n",
       "3       NaN  NaN      5/26/21  \n",
       "4       NaN  NaN       6/2/21  \n",
       "..      ...  ...          ...  \n",
       "321     NaN  NaN      4/23/24  \n",
       "322     NaN  NaN      4/25/24  \n",
       "323     Yes  NaN      4/27/24  \n",
       "324     NaN  NaN      4/27/24  \n",
       "325     NaN  NaN       5/1/24  \n",
       "\n",
       "[326 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from the CSV file and display the first few rows to inspect the data\n",
    "df = pd.read_csv(\"data/diary.csv\")\n",
    "df.head()  # Display the first five rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d7c05",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "Here we manipulate and clean the data. This section includes calling the TMDb API, filtering, handling missing values, and creating new tables, which are essential steps for preparing the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4daebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '53d52c8f3eb74eb1aa0e7b778fc96523' # API key for The Movie Database (TMDb)\n",
    "url_base = \"https://api.themoviedb.org/3/\" # Base URL for TMDb API\n",
    "session = Session() # Create a new session object to make requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdblist = []\n",
    "for index,row in df.iterrows(): \n",
    "    title = row['Name']  # Adjust as necessary to match your DataFrame column name\n",
    "    year = str(row['Year'])  # Convert year to string for comparison\n",
    "    url = f\"{url_base}search/movie?api_key={api_key}&query={title}\"\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        results = data.get('results', [])\n",
    "        if results:  # Check if results list is not empty\n",
    "            # Loop through results to find a matching year\n",
    "            for result in results:\n",
    "                release_date = result.get('release_date', '')\n",
    "                # Extract the year from the release_date\n",
    "                release_year = release_date.split('-')[0] if release_date else ''\n",
    "                if release_year == year: # Check if the release year matches the year in the DataFrame\n",
    "                    movie_data = {\n",
    "                        'movie_title': result.get('title', 'Title not found'),\n",
    "                        'movie_id': result.get('id', 'ID not found'),\n",
    "                        'language': result.get('original_language', 'Language not found'),\n",
    "                        'poster': f\"https://image.tmdb.org/t/p/original'{result.get('poster_path', '')}\",\n",
    "                        'popularity': result.get('popularity', 0),\n",
    "                        'vote_average': result.get('vote_average', 0)\n",
    "                    }\n",
    "                    \n",
    "                    tmdblist.append(movie_data)\n",
    "                    break  # Stop searching after the first match\n",
    "\n",
    "        if not tmdblist:\n",
    "            print(f\"No matching results found for title: {title} in year {year}\")\n",
    "\n",
    "    except (requests.ConnectionError, requests.Timeout, requests.TooManyRedirects) as e:\n",
    "        print(e)\n",
    "\n",
    "print('Finished extracting and matching movies from diary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2444d6b",
   "metadata": {},
   "source": [
    "There were some issues with the data that needed to be addressed:\n",
    "- There were some missing values in the generated tmdb_diary.csv file. So we had to manually look up the missing values and fill them in. The following block fills in the missing values in the tmdb_diary.csv file.\n",
    "\n",
    "Note that I reordered the cells in the notebook to make it easier to follow the data preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff804a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually search and add missing movies to tmdb_diary when compare diary to tmdb_diary left join\n",
    "missing_ids = [\"302104\",\"22970\",\"181886\",\"310131\",\"661374\",\"520763\",\"537116\",\"581734\",\"522627\",\"458156\",\"339967\",\"823464\"]\n",
    "for i in missing_ids: # Loop through the list of movie IDs we noticed were missing\n",
    "    url = f\"{url_base}/movie/{i}?api_key={api_key}&append_to_response=credits\"\n",
    "\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            movie_data = {\n",
    "                'movie_title': data.get('title', 'Title not found'),\n",
    "                'movie_id': data.get('id', 'ID not found'),\n",
    "                'language': data.get('original_language', 'Language not found'),\n",
    "                'poster': f\"https://image.tmdb.org/t/p/original{data.get('poster_path', '')}\",  # Fixed quote placement\n",
    "                'popularity': data.get('popularity', 0),\n",
    "                'vote_average': data.get('vote_average', 0)\n",
    "            }\n",
    "            tmdblist.append(movie_data)\n",
    "            print(f\"Manual entry {movie_data['movie_title']} successful.\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for ID {i}: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {i}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_df = pd.DataFrame(tmdblist)\n",
    "tmdb_df.to_csv('data/generated/tmdb_diary.csv', index=False)\n",
    "tmdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes based on the movie title\n",
    "combined_df = pd.merge(df, tmdb_df, left_on='Name', right_on='movie_title', how='left')\n",
    "\"\"\"\n",
    "Left Join because the important data here is our own diary.\n",
    "This is also where I did some manual data cleaning because\n",
    "the search function created wrong entries in `tmdb_diary.csv`,\n",
    "so I had to search these values up manually. Some values simply\n",
    "fall through the cracks.\n",
    "\n",
    "In the next iteration of this project, I would like to implement\n",
    "a better search function that is more robust and can handle\n",
    "more edge cases. Perhaps joining on the movie title is not the\n",
    "best way to do this, but I couldn't find a better way to do it based\n",
    "on the data I had available.\n",
    "\"\"\"\n",
    "# # Drop duplicates with a specific condition that ignores valid re-watches\n",
    "combined_df = combined_df.drop_duplicates(subset=['Name', 'movie_id', 'Watched Date'], keep='first')\n",
    "# Save the cleaned-up DataFrame\n",
    "combined_df.to_csv('data/generated/combined_diaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd9d09",
   "metadata": {},
   "source": [
    "Note: Remember to manually browse through the generated combined entries to ensure that the combined entries are correctly filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = pd.read_csv('data/generated/combined_diaries.csv')\n",
    "working_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac95ddc",
   "metadata": {},
   "source": [
    "Let us now make another API call to get additional information about the movies in our dataset. We will use the movie_id from the previous API call to get more detailed information about each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bba9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list\n",
    "ids = []\n",
    "\n",
    "# Loop through 'movie_id' in working_data and add each to the set\n",
    "for unique_id in working_data['movie_id']:\n",
    "    ids.append(unique_id)\n",
    "\n",
    "# print(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861ca29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "addtl_tmdb_list = []\n",
    "for uid in ids:\n",
    "    # https://api.themoviedb.org/3/movie/1700?api_key=2ca28daf3aadbe02b38f276d24d29e8e&append_to_response=credits\n",
    "    url = f\"{url_base}/movie/{uid}?api_key={api_key}&append_to_response=credits\"\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            genres = [genre['name'] for genre in data['genres']]\n",
    "            addtl_movie_data = {\n",
    "                'movie_title': data.get('title', 'Title not found'),\n",
    "                'movie_id': data.get('id', 'ID not found'),\n",
    "                'release_date': data.get('release_date', 'Release date not found'),\n",
    "                'genres' : genres,\n",
    "                'origin_country': data.get('origin_country', []),\n",
    "                'revenue': data.get('revenue', 0),\n",
    "                'runtime': data.get('runtime', 'Not specified'),\n",
    "                'tagline': data.get('tagline', 'No tagline'),\n",
    "                'cast': [{'name': member.get('name', 'Unknown'), 'gender': member.get('gender', 'Not specified')} for member in data.get('credits', {}).get('cast', [])],\n",
    "                'director': [crew_member.get('name', 'Unknown') for crew_member in data.get('credits', {}).get('crew', []) if crew_member.get('job') == 'Director']\n",
    "            }\n",
    "            addtl_tmdb_list.append(addtl_movie_data)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for ID {uid}: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {uid}: {str(e)}\")\n",
    "print('Fetch Additional Details Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70061e",
   "metadata": {},
   "source": [
    "Let's now export the additional information to a CSV file for further analysis and to ensure that we have a backup of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dictionaries to DataFrame\n",
    "addtl_tmdb_df = pd.DataFrame(addtl_tmdb_list)\n",
    "\n",
    "# Drop duplicates with a specific condition that ignores valid re-watches\n",
    "addtl_tmdb_df = addtl_tmdb_df.drop_duplicates(subset=['movie_title','movie_id'], keep='first')\n",
    "# Save the cleaned-up DataFrame\n",
    "addtl_tmdb_df.to_csv('data/generated/addtl_tmdb_diaries.csv', index=False)\n",
    "\n",
    "\n",
    "print('Finished extracting addtl_tmdb_diary.csv details!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249f845",
   "metadata": {},
   "source": [
    "# Data Consolidation\n",
    "\n",
    "Let's now combine the additional information with the existing data to create a consolidated dataset for further analysis.\n",
    "\n",
    "Let us also standardize column names and data types to ensure consistency in the dataset, remove columns that are not needed, and handle incorrect values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aeec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge working_data and addtl_tmdb_df using left join\n",
    "working_combined_df = pd.merge(working_data, addtl_tmdb_df, how='left', left_on='Name', right_on='movie_title')\n",
    "\n",
    "# Replace 'Name' column with 'movie_title_x' values\n",
    "working_combined_df['Name'] = working_combined_df['movie_title_x']\n",
    "\n",
    "# Drop 'Date', 'Tags', 'movie_title_x', 'movie_title_y' columns\n",
    "columns_to_drop = ['Date', 'Tags', 'movie_title_x', 'movie_title_y','movie_id_y']\n",
    "working_combined_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Rename some columns\n",
    "working_combined_df = working_combined_df.rename(columns={'movie_id_x': 'movie_id'})\n",
    "\n",
    "# Standardize column names (convert to lowercase and replace spaces with underscores)\n",
    "working_combined_df.columns = [col.lower().replace(' ', '_') for col in working_combined_df.columns]\n",
    "\n",
    "# Convert 'watched_date' and 'release_date' to datetime format\n",
    "# Might need to reformat some entries manually because on release_date, 48 == 1948, != 2048\n",
    "working_combined_df['watched_date'] = pd.to_datetime(working_combined_df['watched_date'])\n",
    "working_combined_df['release_date'] = pd.to_datetime(working_combined_df['release_date'])\n",
    "\n",
    "# Order by 'watched_date'\n",
    "working_combined_df.sort_values(by='watched_date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99910f9d",
   "metadata": {},
   "source": [
    "We can use this consolidated dataset for further analysis and visualization through Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e403437",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_combined_df.to_csv('data/generated/working_combined_diaries.csv', index=False)\n",
    "print(\"f\")\n",
    "# This is the final csv for staging and EDA in Python. Left Joined on our diary.csv, and added additional information from tmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515691e",
   "metadata": {},
   "source": [
    "# Data Preparation for Tableau\n",
    "\n",
    "We should also create a version of the dataset that will play nicely with Tableau. Especially since Tableau is not as flexible as Python in handling lists and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ddf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/generated/working_combined_diaries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2066b",
   "metadata": {},
   "source": [
    "The following block generates the df that we'll later split into separate tables for genre, country, and director list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split and clean a list string, handling NaN values, with dynamic column creation\n",
    "def split_list_string(list_string, prefix):\n",
    "    if pd.isna(list_string):\n",
    "        return {}\n",
    "    # Remove square brackets and split by comma\n",
    "    elements = list_string.strip(\"[]\").replace(\"'\", \"\").split(\", \")\n",
    "    return {f\"{prefix}_{i+1}\": elements[i] for i in range(len(elements))}\n",
    "\n",
    "# Apply the function to the 'genres' column\n",
    "genres_df = df['genres'].apply(split_list_string, prefix='genre').apply(pd.Series)\n",
    "df = df.drop(columns=['genres'])\n",
    "df = pd.concat([df, genres_df], axis=1)\n",
    "\n",
    "# Apply the function to the 'origin_country' column\n",
    "origin_country_df = df['origin_country'].apply(split_list_string, prefix='country').apply(pd.Series)\n",
    "df = df.drop(columns=['origin_country'])\n",
    "df = pd.concat([df, origin_country_df], axis=1)\n",
    "\n",
    "# Apply the function to the 'director' column\n",
    "director_df = df['director'].apply(split_list_string, prefix='director').apply(pd.Series)\n",
    "df = df.drop(columns=['director'])\n",
    "df = pd.concat([df, director_df], axis=1)\n",
    "\n",
    "# Remove columns with all NaN values\n",
    "df = df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b10f55",
   "metadata": {},
   "source": [
    "Let's add a column for the entry_id to the df so that we can join the tables later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcdf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a unique id for each entry with standardized format of 5 digits\n",
    "df['entry_id'] = df.index + 1\n",
    "df['entry_id'] = df['entry_id'].apply(lambda x: f\"{x:05d}\")\n",
    "\n",
    "# Add the id column to the beginning of the DataFrame\n",
    "df = pd.concat([df['entry_id'], df.drop(columns=['entry_id'])], axis=1)\n",
    "df\n",
    "\n",
    "df.to_csv('data/generated/final_combined_diaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827d194",
   "metadata": {},
   "source": [
    "The following block splits the df into separate tables for genre, country, and director list items. This is necessary for creating a relational database in Tableau without having to pivot through Tableau, making it easier to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90353b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate table for genres where the values are pivoted; three columns only: entry id, genre number and genre sorted by entry id and genre number\n",
    "df_entries_and_genres = df[['id','genre_1','genre_2','genre_3','genre_4','genre_5']].melt(id_vars='id', value_name='genre').dropna()\n",
    "df_entries_and_genres['genre_number'] = df_entries_and_genres['variable'].str.extract(r'genre_(\\d+)')\n",
    "df_entries_and_genres = df_entries_and_genres.drop(columns='variable')\n",
    "df_entries_and_genres = df_entries_and_genres.sort_values(by=['id','genre_number'])\n",
    "df_entries_and_genres\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_entries_and_genres.to_csv('data/generated/entries_and_genres.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate table for countries where the values are pivoted; three columns only: entry id, country number and country sorted by entry id and country number\n",
    "df_entries_and_countries = df[['id','country_1','country_2','country_3']].melt(id_vars='id', value_name='country').dropna()\n",
    "df_entries_and_countries['country_number'] = df_entries_and_countries['variable'].str.extract(r'country_(\\d+)')\n",
    "df_entries_and_countries = df_entries_and_countries.drop(columns='variable')\n",
    "df_entries_and_countries = df_entries_and_countries.sort_values(by=['id','country_number'])\n",
    "df_entries_and_countries\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_entries_and_countries.to_csv('data/generated/entries_and_countries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87feee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate table for directors where the values are pivoted; three columns only: entry id, director number and director sorted by entry id and director number\n",
    "df_entries_and_directors = df[['id','director_1','director_2','director_3']].melt(id_vars='id', value_name='director').dropna()\n",
    "df_entries_and_directors['director_number'] = df_entries_and_directors['variable'].str.extract(r'director_(\\d+)')\n",
    "df_entries_and_directors = df_entries_and_directors.drop(columns='variable')\n",
    "df_entries_and_directors = df_entries_and_directors.sort_values(by=['id','director_number'])\n",
    "df_entries_and_directors\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_entries_and_directors.to_csv('data/generated/entries_and_directors.csv', index=False)\n",
    "\n",
    "# These generated files will be used to create the database schema and tables for the project's final data visualization and analysis in Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e96b9f",
   "metadata": {},
   "source": [
    "Our data is now ready for further analysis and visualization in Tableau."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
